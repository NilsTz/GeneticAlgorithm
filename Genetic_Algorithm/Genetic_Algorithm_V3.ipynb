{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Nils\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Nils\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Nils\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Nils\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Nils\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Nils\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from keras.optimizers import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Individual:\n",
    "    \n",
    "    def __init__(self, learn_rate, epochs, batch, neurons_dense, activation, optimizer):\n",
    "        \n",
    "        self.learn_rate = learn_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch = batch          \n",
    "        self.neurons_dense = neurons_dense\n",
    "       \n",
    "        self.activation = activation\n",
    "        self.optimizer = optimizer\n",
    "      \n",
    "        self.loss = 0\n",
    "        self.acc = 0\n",
    "        self.prec = 0\n",
    "        self.rec = 0\n",
    "        self.f1 = 0\n",
    "        self.generation = 0\n",
    "        self.time = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class readGeneration:\n",
    "    \n",
    "    def __init__(self, learn_rate, epochs, batch, neurons_dense, activation, optimizer, loss, acc, \n",
    "                 prec, rec, f1, generation, time):\n",
    "        \n",
    "        self.learn_rate = learn_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch = batch          \n",
    "        self.neurons_dense = neurons_dense\n",
    "       \n",
    "        self.activation = activation\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "      \n",
    "        self.loss = loss\n",
    "        self.acc = acc\n",
    "        self.prec = prec\n",
    "        self.rec = rec\n",
    "        self.f1 = f1\n",
    "        self.generation = generation\n",
    "        self.time = time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show (pop):\n",
    "    for i in range(len(pop)):\n",
    "        print(\"INDIVIDUUM:\",i+1)\n",
    "        print(\"Lernrate:\",pop[i].learn_rate, \"Epochen:\",pop[i].epochs, \"Batch:\",pop[i].batch,)\n",
    "        print(\"Aktivfkt:\", pop[i].activation, \"Optimizer:\", pop[i].optimizer)\n",
    "        print(\"Anzahl dense:\", pop[i].neurons_dense)\n",
    "        print(\"Acc:\", pop[i].acc, \"Loss:\", pop[i].loss, \"f1:\", pop[i].f1, \"Generation:\", pop[i].generation, '\\n \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_population(pop_count):\n",
    "    \n",
    "    pop =[]\n",
    "    activation_list = ['elu', 'selu','relu','tanh','sigmoid', 'hard_sigmoid','exponential', 'linear', \n",
    "                    'softmax','softplus','softsign']\n",
    "    optimizer_list = [\"SGD\", \"RMSprop\", \"Adagrad\", \"Adadelta\", \"Adam\", \"Adamax\", \"Nadam\"]\n",
    "\n",
    "    activation_list_pop = []\n",
    "    optimizer_list_pop = []\n",
    "  \n",
    "    folds = int(pop_count/77)\n",
    "    for j in range(0,folds) :   \n",
    "        for k in range(len(activation_list)):\n",
    "            for n in range(len(optimizer_list)):\n",
    "                activation_list_pop.append(activation_list[k])\n",
    "                optimizer_list_pop.append(optimizer_list[n])\n",
    "            \n",
    "    for i in range(0,pop_count):\n",
    "        learn_rate    = round(random.uniform(0.00001,0.1),5)\n",
    "        epochs        = random.randrange(1,64)\n",
    "        batch         = random.randrange(1,512)        \n",
    "    \n",
    "        neurons_dense  = random.randrange(1,1200)\n",
    "        \n",
    "        activation  = activation_list_pop[i]\n",
    "        optimizer = optimizer_list_pop[i]\n",
    "                        \n",
    "        pop.append(Individual(learn_rate, epochs, batch, neurons_dense, activation, optimizer))\n",
    "        \n",
    "    return pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_fittest(pop):\n",
    "    parents= []\n",
    "    \n",
    "    pop.sort(key = lambda x:x.acc, reverse = True)\n",
    "    \n",
    "    good_half = pop[:len(pop)//2]\n",
    "       \n",
    "    return good_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breed_new(pop, pop_count):\n",
    "    children = []\n",
    "\n",
    "    for i in range(0,pop_count):\n",
    "        while True: \n",
    "            k = random.randrange(0,len(pop)//2) \n",
    "            l = random.randrange(0,len(pop))   \n",
    "            if k != l:\n",
    "                break\n",
    "        parent_1 = pop[k]\n",
    "        parent_2 = pop[l]\n",
    "        \n",
    "        child_lr = random.choice([parent_1.learn_rate, parent_2.learn_rate])\n",
    "        child_epo = random.choice([parent_1.epochs, parent_2.epochs])\n",
    "        child_bat = random.choice([parent_1.batch, parent_2.batch])        \n",
    "        child_dense = random.choice([parent_1.neurons_dense, parent_2.neurons_dense])\n",
    "        \n",
    "        child_activation = random.choice([parent_1.activation, parent_2.activation])\n",
    "        child_optimizer = random.choice([parent_1.optimizer, parent_2.optimizer])\n",
    "    \n",
    "        children.append(Individual(child_lr, child_epo, child_bat, \n",
    "                                   child_dense,\n",
    "                                   child_activation, child_optimizer))   \n",
    "    \n",
    "    return children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate_new(pop, max_mutation, mutation_rate):\n",
    "    \n",
    "    mutated = []\n",
    "    \n",
    "    activation_list = ['elu', 'selu','relu','tanh','sigmoid', 'hard_sigmoid','exponential', 'linear', \n",
    "                      'softmax','softplus','softsign']\n",
    "    optimizer_list = [\"SGD\", \"RMSprop\", \"Adagrad\", \"Adadelta\", \"Adam\", \"Adamax\", \"Nadam\"]\n",
    "    \n",
    "    mutation_flag = get_flag(len(pop),mutation_rate) \n",
    "    \n",
    "    if max_mutation == 0:\n",
    "    \n",
    "        return pop\n",
    "    else:\n",
    "        for i in range(len(pop)):\n",
    "            if mutation_flag[i] == 1:\n",
    "               \n",
    "                mutated_lr = round(pop[i].learn_rate + pop[i].learn_rate* round(random.uniform(-max_mutation,max_mutation),3),5)\n",
    "                mutated_epochs = int(round(pop[i].epochs + pop[i].epochs*round(random.uniform(-max_mutation,max_mutation),3),0))\n",
    "                mutated_batch = int(round(pop[i].batch + pop[i].batch*round(random.uniform(-max_mutation,max_mutation),3),0))\n",
    "                \n",
    "                mutated_neurons_dense = int(round(pop[i].neurons_dense + pop[i].neurons_dense*round(random.uniform(-max_mutation,max_mutation),3),0))\n",
    "               \n",
    "                nominal_mutation_flag = get_flag(1, mutation_rate) \n",
    "                \n",
    "                if nominal_mutation_flag[0] == 1:\n",
    "                    mutated_activation = random.choice(activation_list) \n",
    "                    mutated_optimizer =  random.choice(optimizer_list)\n",
    "                else:\n",
    "                    mutated_activation = pop[i].activation\n",
    "                    mutated_optimizer = pop[i].optimizer\n",
    "                \n",
    "            \n",
    "                mutated.append(Individual(mutated_lr, mutated_epochs, mutated_batch,\n",
    "                                          mutated_neurons_dense,\n",
    "                                          mutated_activation, mutated_optimizer))\n",
    "            else:\n",
    "                mutated.append(pop[i])\n",
    "    return mutated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_boundaries(pop):\n",
    "    for i in range(len(pop)):\n",
    "        if not 0.00001 <= pop[i].learn_rate <= 0.1: \n",
    "            if pop[i].learn_rate > 0.05:\n",
    "                pop[i].learn_rate = 0.1\n",
    "            else:\n",
    "                pop[i].learn_rate = 0.00001\n",
    "            \n",
    "        if not 1 <= pop[i].epochs <= 128:\n",
    "            if pop[i].epochs > 100:\n",
    "                pop[i].epochs = 128\n",
    "            else:\n",
    "                pop[i].epochs = 1\n",
    "        \n",
    "        if not 1 <= pop[i].batch <= 512:\n",
    "            if pop[i].batch > 500:\n",
    "                pop[i].batch = 512\n",
    "            else:\n",
    "                pop[i].batch = 1\n",
    "        \n",
    "        if not 1 <= pop[i].neurons_dense <= 1200:\n",
    "            if pop[i].neurons_dense > 1000:\n",
    "                pop[i].neurons_dense = 1200\n",
    "            else:\n",
    "                pop[i].neurons_dense = 1\n",
    "                \n",
    "    return pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(optimizer, learn_rate):\n",
    "    if optimizer == 'SGD':\n",
    "        rtr_optimizer = SGD(lr = learn_rate)\n",
    "        \n",
    "    if optimizer == 'RMSprop':\n",
    "        rtr_optimizer = RMSprop(lr = learn_rate)\n",
    "        \n",
    "    if optimizer == 'Adagrad':\n",
    "        rtr_optimizer = Adagrad(lr = learn_rate)\n",
    "        \n",
    "    if optimizer == 'Adadelta':\n",
    "        rtr_optimizer = Adadelta(lr = learn_rate)\n",
    "        \n",
    "    if optimizer == 'Adam':\n",
    "        rtr_optimizer = Adam(lr = learn_rate)\n",
    "        \n",
    "    if optimizer == 'Adamax':\n",
    "        rtr_optimizer = Adamax(lr = learn_rate)\n",
    "        \n",
    "    if optimizer == 'Nadam':\n",
    "        rtr_optimizer = Nadam(lr = learn_rate)\n",
    "           \n",
    "    return rtr_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flag(length,mutation_rate):\n",
    "    list_flag = []\n",
    "    a = np.array([0,1])\n",
    "    probability = np.array([1-mutation_rate,mutation_rate])\n",
    "    for i in range(0,length):\n",
    "        flag = np.random.choice(a,p= probability)\n",
    "        list_flag.append(flag)\n",
    "    return list_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_generation(pop, generation):\n",
    "    for i in range(len(pop)): \n",
    "        pop[i].generation = generation\n",
    "    return pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_max(pop, best):\n",
    "    for i in range(len(pop)):\n",
    "        if pop[i].acc > best.acc:\n",
    "            best = pop[i]\n",
    "    return best    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file_jupyter (pop, filename):\n",
    "    list_lr = []\n",
    "    list_epochs = []\n",
    "    list_batch =[]\n",
    "    \n",
    "    list_neurons_dense = []    \n",
    "    \n",
    "    list_activation = []\n",
    "    list_optimizer = []\n",
    "    \n",
    "    list_loss = []\n",
    "    list_acc = []\n",
    "    list_prec = []\n",
    "    list_rec = []\n",
    "    list_f1 = []\n",
    "    list_generation = []\n",
    "    list_time = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(pop)):\n",
    "        list_lr.append(pop[i].learn_rate)\n",
    "        list_epochs.append(pop[i].epochs)\n",
    "        list_batch.append(pop[i].batch)\n",
    "        \n",
    "        list_neurons_dense.append(pop[i].neurons_dense)\n",
    "        \n",
    "        list_activation.append(pop[i].activation)\n",
    "                \n",
    "        list_optimizer.append(pop[i].optimizer)\n",
    "               \n",
    "        list_loss.append(pop[i].loss)\n",
    "        list_acc.append(pop[i].acc)\n",
    "        list_prec.append(pop[i].prec)\n",
    "        list_rec.append(pop[i].rec)\n",
    "        list_f1.append(pop[i].f1)\n",
    "        list_generation.append(pop[i].generation)\n",
    "        list_time.append(pop[i].time)\n",
    "        \n",
    "        \n",
    "    data_out = pd.DataFrame(list(zip(list_lr, list_epochs, list_batch,\n",
    "                                     list_neurons_dense, list_activation, list_optimizer,\n",
    "                                     list_loss, list_acc, list_prec, list_rec,\n",
    "                                     list_f1, list_generation, list_time)))\n",
    "    header = [\"Learn_rate\", \"Epochs\", \"Batch\",\n",
    "              \"Neurons_Dense\", \"Activation\",\"Optimizer\",\n",
    "              \"Loss\",\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"Generation\",\"Zeitstempel\"]\n",
    "    data_out.to_csv(filename, sep =';', header = header, decimal='.', float_format ='%.5f', mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file_excel (pop, filename):\n",
    "    list_lr = []\n",
    "    list_epochs = []\n",
    "    list_batch =[]\n",
    "    \n",
    "    list_neurons_dense = []    \n",
    "    \n",
    "    list_activation = []\n",
    "    list_optimizer = []\n",
    "    \n",
    "    list_loss = []\n",
    "    list_acc = []\n",
    "    list_prec = []\n",
    "    list_rec = []\n",
    "    list_f1 = []\n",
    "    list_generation = []\n",
    "    list_time = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(pop)):\n",
    "        list_lr.append(pop[i].learn_rate)\n",
    "        list_epochs.append(pop[i].epochs)\n",
    "        list_batch.append(pop[i].batch)\n",
    "        \n",
    "        list_neurons_dense.append(pop[i].neurons_dense)\n",
    "        \n",
    "        list_activation.append(pop[i].activation)\n",
    "                \n",
    "        list_optimizer.append(pop[i].optimizer)\n",
    "               \n",
    "        list_loss.append(pop[i].loss)\n",
    "        list_acc.append(pop[i].acc)\n",
    "        list_prec.append(pop[i].prec)\n",
    "        list_rec.append(pop[i].rec)\n",
    "        list_f1.append(pop[i].f1)\n",
    "        list_generation.append(pop[i].generation)\n",
    "        list_time.append(pop[i].time)\n",
    "        \n",
    "        \n",
    "    data_out = pd.DataFrame(list(zip(list_lr, list_epochs, list_batch,\n",
    "                                     list_neurons_dense, list_activation, list_optimizer,\n",
    "                                     list_loss, list_acc, list_prec, list_rec,\n",
    "                                     list_f1, list_generation, list_time)))\n",
    "    header = [\"Learn_rate\", \"Epochs\", \"Batch\",\n",
    "              \"Neurons_Dense\", \"Activation\",\"Optimizer\",\n",
    "              \"Loss\",\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"Generation\",\"Zeitstempel\"]\n",
    "    data_out.to_csv(filename, sep =';', header = header, decimal=',', float_format ='%.5f', mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_file(filename):\n",
    "    pop = []\n",
    "    with open (filename, 'r') as f:\n",
    "        read_data = csv.reader(f, delimiter =\";\")\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            splitted_line = line.split(';')\n",
    "            learn_rate    = float(splitted_line[1])\n",
    "            epochs        = int(splitted_line[2])\n",
    "            batch         = int(splitted_line[3])\n",
    "            neurons_dense = int(splitted_line[4])\n",
    "            activation    = splitted_line[5]\n",
    "            optimizer     = splitted_line[6]\n",
    "            loss          = float(splitted_line[7])\n",
    "            acc           = float(splitted_line[8])\n",
    "            prec          = float(splitted_line[9])\n",
    "            rec           = float(splitted_line[10])\n",
    "            f1            = float(splitted_line[11])\n",
    "            generation    = int(splitted_line[12])\n",
    "            timestamp     = 0#float(splitted_line[13])\n",
    "\n",
    "            pop.append(readGeneration(learn_rate, epochs, batch, neurons_dense, activation, optimizer, loss,\n",
    "                                      acc, prec, rec, f1, generation, timestamp))\n",
    "        return pop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
